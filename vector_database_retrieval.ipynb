{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import json\n",
    "import chromadb\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_PATH = 'embeddings'\n",
    "MODEL_NAME = 'llama3-70b-8192'\n",
    "FILENAME = 'data/un_regulations_157.json'\n",
    "RESPONSE_TEMPLATE = \"\"\"The context data contains a automotive compliance regulations, most of the time its either a requirement or a definition.\n",
    "The regulations given in the context are identified by their id located at the beginning and followed by a colon (3.2.1.:, 2.4.: etc...). Include regulation id in your answer whenever necessary.\n",
    "Use the following pieces of context to answer the query at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "question = 'Which requirements should be met for the vehicle type submitted for approval?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector datastore RAG\n",
    "![title](images/rag_vector_store.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME) as f:\n",
    "    data = json.load(f)\n",
    "reg_ids = [regulation.splitlines()[0].strip() for regulation in data]\n",
    "\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "collection = client.get_or_create_collection(\n",
    "    name='current_db',\n",
    "    metadata={'hnsw:space': 'cosine'})\n",
    "collection.upsert(\n",
    "    documents=data,\n",
    "    ids=reg_ids\n",
    "    )\n",
    "llm_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is often subjective and can vary greatly from person to person, but at its core, it is the pursuit of happiness, fulfillment, and personal growth through the connections we make, the experiences we have, and the contributions we make to the world around us. Ultimately, the meaning of life is a journey of self-discovery, acceptance, and wisdom, and it is up to each individual to define what gives their life significance and purpose.\n"
     ]
    }
   ],
   "source": [
    "chat_completion = llm_client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the meaning of life in 2 sentences\",\n",
    "        }\n",
    "    ],\n",
    "    model=MODEL_NAME,\n",
    ")\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(msg, top_k=3):\n",
    "    query_results = collection.query(\n",
    "        query_texts=[msg],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return query_results[\"documents\"][0]\n",
    "\n",
    "def get_completion(msg, context, max_new_tokens, temperature, top_k):\n",
    "    # context = retrieve(msg, top_k)\n",
    "    query = RESPONSE_TEMPLATE.format(context=context, question=msg)\n",
    "    chat_completion = llm_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_new_tokens,\n",
    "        model=MODEL_NAME,\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def generate_qa(msg, max_new_tokens, temperature, top_k):\n",
    "    retrieve_result = retrieve(msg, top_k)\n",
    "    print(f'Retrieved result from the vectorstore: {retrieve_result}')\n",
    "    result = get_completion(msg, retrieve_result, max_new_tokens, temperature, top_k)\n",
    "    return result\n",
    "\n",
    "def generate_chat(msg, max_new_tokens, temperature, top_k, chat_history):\n",
    "    retrieve_result = retrieve(msg, top_k)\n",
    "    result = get_completion(msg, retrieve_result, max_new_tokens, temperature, top_k)\n",
    "    chat_history.append((msg, result))\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://13f42b84f4dc8643ff.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://13f42b84f4dc8643ff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved result from the vectorstore: ['10.: Modification of vehicle type and extension of type approval', '10.\\nModification of vehicle type and extension of type approval', '10.1.\\nEvery modification to an existing vehicle type shall be notified to the Type Approval Authority which approved the vehicle type.\\nThe Authority shall then either:\\n(a)\\nDecide, in consultation with the manufacturer, that a new type-approval is to be granted; or\\n(b)\\nApply the procedure contained in paragraph 10.1.1 (Revision) and, if applicable, the procedure contained in paragraph 10.1.2 (Extension). 10.1.\\nModel of the information provided to users (including expected driver’s tasks within the ODD and when going out of the ODD) …']\n"
     ]
    }
   ],
   "source": [
    "qa_interface = gr.Interface(fn=generate_qa, \n",
    "                            title=\"QA interface\",\n",
    "                            inputs=[gr.Textbox(label=\"Prompt\"),\n",
    "                                    gr.Slider(label=\"Max new tokens\", value=200, maximum=1024, step=int, minimum=1),\n",
    "                                    gr.Slider(label=\"Temperature\", value=0.8, maximum=1.0, minimum=0.0),\n",
    "                                    gr.Slider(label=\"Max documents retrieved\", value=3, step=int, maximum=5, minimum=1)\n",
    "                                   ], \n",
    "                            outputs=[gr.Textbox(label=\"Completion\")]\n",
    "                           )\n",
    "gr.close_all()\n",
    "qa_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moz9le/Documents/ml11/lib/python3.11/site-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://880b55523f5c107cb2.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://880b55523f5c107cb2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as chat_interface:\n",
    "    title = gr.HTML(\"<center><h1>Chatbot</h1></center>\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(label=\"Prompt\")\n",
    "            max_new_tokens = gr.Slider(label=\"Max new tokens\", value=200, maximum=1024, step=int, minimum=1)\n",
    "            temperature = gr.Slider(label=\"Temperature\", value=0.8, maximum=1.0, minimum=0.0)\n",
    "            max_documents = gr.Slider(label=\"Max documents retrieved\", value=3, step=int, maximum=5, minimum=1)\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(height=240)\n",
    "    btn = gr.Button(\"Query\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "    btn.click(\n",
    "        generate_chat,\n",
    "        inputs=[\n",
    "            msg,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            max_documents,\n",
    "            chatbot],\n",
    "        outputs=[msg, chatbot])\n",
    "gr.close_all()\n",
    "chat_interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
