{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdbf58e-8082-47ae-9456-e234fc510252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from groq import Groq\n",
    "import gradio as gr\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e518810-7fe7-4e93-9a57-c9be29d107b0",
   "metadata": {},
   "source": [
    "# Graph RAG\n",
    "![title](images/graph_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3ccf9-a1d1-4ac2-b70e-9fde3ae1ec1c",
   "metadata": {},
   "source": [
    "## Load model and prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23afe5eb-d36b-42ad-9940-4309cb7eb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'data/un_regulations_157_graph.json'\n",
    "USERNAME = 'neo4j'\n",
    "URI = 'neo4j://localhost:7687'\n",
    "MODEL_NAME = 'llama3-70b-8192'\n",
    "\n",
    "NODE_SELECTION = \"\"\"Goal:Identify the most relevant node.\n",
    "Context:\n",
    "The graph database contains a list of automotive regulation compliance text, each node represents a requirement or a definition.\n",
    "Each node has a 'text' attribute which contains the actual regulation.\n",
    "The relationships represent either a hierarchical relationship (node 3.2.1 is child_of 3.2) or a mention relation (node 4.2.5.1 mentions regulation 2.1.0)\n",
    "Input:\n",
    "- question: User's query\n",
    "- list_of_nodes: A dict of relevant nodes w.r.t. user query, where the key is the node_id and the value node_text refers to the description of the node\n",
    "Task: Your task is to identify the node thats most relevant for the input query.\n",
    "Instructions:\n",
    "- From the input nodes identify one node thats most relevant for the user's query (called n1) by analysing the relevance of the text to the query\n",
    "- After having conducted the analysis, report only the end result and not the breakdown of the analysis itself.\n",
    "- Return the id of the node in a yaml format with key 'output'\n",
    "Note:\n",
    "- Do not return anything else except the yaml output as the result shall be used by an automated code.\n",
    "Example:\n",
    "```\n",
    "Question:\n",
    "    \"What are the procedures to take to ensure mechanical safety?\"\n",
    "List of nodes:\n",
    "{{\n",
    "    '4.3.1': \"A transition demand shall not endanger the safety of the vehicle occupants or other road users.\",\n",
    "    '9.3.1': \"Most guidelines relating to vehicle inspection and safety are referred to in 3.7.2\",\n",
    "    '3.5.4.2': \"Procedures manual for a safe sotware updates are included below. This does not include mechanical activations\"\n",
    "}}\n",
    "// it can be seen that '9.3.1' is the most relevant to the user's query\n",
    "Output: '9.3.1'\n",
    "```\n",
    "OK, now here's the input\n",
    "Question:\n",
    "{query}\n",
    "List_of_nodes:\n",
    "{list_of_nodes}\n",
    "Output:\n",
    "\"\"\"\n",
    "\n",
    "QA_PROMPT_TEMPLATE = \"\"\"Task: Formulate an extensive answer to the question given the returned context and the user's query.\n",
    "The context data contains a automotive compliance regulations, most of the time its either a requirement or a definition.\n",
    "The regulations given in the context are identified by their id located at the beginning and followed by a colon (3.2.1.:, 2.4.: etc...). Include regulation id in your answer whenever necessary.\n",
    "Use the following pieces of context to answer the query at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\n",
    "\"\"\"\n",
    "\n",
    "QA_GENERATION_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=QA_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3343e69d-49df-4137-82f1-badc41df4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FILENAME) as f:\n",
    "    data = json.load(f)\n",
    "all_nodes = [node[\"id\"] for node in data]\n",
    "graph = Neo4jGraph(url=URI, username=USERNAME, password=os.environ.get(\"NEO4J_PWD\"))\n",
    "llm_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=1024,\n",
    "    api_key=os.environ.get('OPEN_AI_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905d3400-c4d2-466b-875d-ecdf71f7b42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node properties:\n",
      "Node {id: STRING, text: STRING, embeddings: LIST}\n",
      "Relationship properties:\n",
      "\n",
      "The relationships:\n",
      "(:Node)-[:child_of]->(:Node)\n",
      "(:Node)-[:mentions]->(:Node)\n"
     ]
    }
   ],
   "source": [
    "with open(FILENAME) as f:\n",
    "    data = json.load(f)\n",
    "all_nodes = [node[\"id\"] for node in data]\n",
    "graph = Neo4jGraph(url=URI, username=USERNAME, password=os.environ.get(\"NEO4J_PWD\"))\n",
    "llm_client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "embeddings_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    dimensions=1024,\n",
    "    api_key=os.environ.get('OPEN_AI_KEY')\n",
    ")\n",
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd533c-14ad-4c61-94fd-d346e88adce3",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a1cb541-7a70-4671-ab2b-7c4d37c16707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_nodes(question, top_k):\n",
    "    fetch_top_nodes=\"\"\"WITH $query_embedding AS queryEmbedding\n",
    "    MATCH (n:Node)\n",
    "    WITH n, \n",
    "        reduce(dot = 0.0, i IN range(0, size(n.embeddings) - 1) | dot + n.embeddings[i] * queryEmbedding[i]) AS dot_product,\n",
    "        reduce(norm1 = 0.0, i IN range(0, size(n.embeddings) - 1) | norm1 + n.embeddings[i] ^ 2) AS norm1,\n",
    "        reduce(norm2 = 0.0, i IN range(0, size(queryEmbedding) - 1) | norm2 + queryEmbedding[i] ^ 2) AS norm2\n",
    "    WITH n, dot_product / (sqrt(norm1) * sqrt(norm2)) AS similarity\n",
    "    WHERE similarity > 0.0 // Optional: Filter negative similarities if desired\n",
    "    ORDER BY similarity DESC\n",
    "    RETURN n.id AS id, n.text as text, similarity as sim\n",
    "    LIMIT $top_k;\n",
    "    \"\"\"\n",
    "    query_embedding = embeddings_model.embed_query(question)\n",
    "    similar_nodes = graph.query(fetch_top_nodes, {\"query_embedding\": query_embedding, \"top_k\":top_k})\n",
    "    print(f'1. Retrieve data from vector store based on similarity:\\nSimilar nodes to the query {[(result[\"id\"], result[\"sim\"]) for result in similar_nodes]}')\n",
    "    return {result['id']: result['text'] for result in similar_nodes}\n",
    "\n",
    "def get_most_relevant_node(question, similar_nodes_dict):\n",
    "    query = NODE_SELECTION.format(query=question, list_of_nodes=similar_nodes_dict)\n",
    "    chat_completion = llm_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        model=MODEL_NAME,\n",
    "    )\n",
    "    completion = chat_completion.choices[0].message.content\n",
    "    most_relevant_node = ''\n",
    "    list_tokens = completion.split(' ')\n",
    "    for token in list_tokens:\n",
    "        token = token.replace('\"', '')\n",
    "        token = token.replace(\"'\", \"\")\n",
    "        if token in all_nodes:\n",
    "            most_relevant_node = token\n",
    "            break\n",
    "    if most_relevant_node == '':\n",
    "        print('LLM failed to return a valid node')\n",
    "        most_relevant_node = list(similar_nodes_dict.keys())[0]\n",
    "    most_relevant_txt = similar_nodes_dict[most_relevant_node]\n",
    "    print(f'2. LLM based decision on the most relevant nodeMost relevant node {most_relevant_node}: {most_relevant_txt}')\n",
    "    return most_relevant_node, most_relevant_txt\n",
    "\n",
    "def build_context(most_relevant_node, most_relevant_txt):\n",
    "    fetch_related_nodes = f\"\"\"\n",
    "    MATCH (n:Node {{id: '{most_relevant_node}'}})-[:mentions]->(mentionedNode:Node)\n",
    "    RETURN mentionedNode.id AS id, mentionedNode.text AS text;\n",
    "    \"\"\"\n",
    "    related_nodes = graph.query(fetch_related_nodes)\n",
    "    print(f'3. Cypher query to generate the full context based on the most relevant node: {related_nodes}')\n",
    "\n",
    "    # answer the query\n",
    "    full_context = [most_relevant_node + ': ' + most_relevant_txt]\n",
    "    for related_node in related_nodes:\n",
    "        full_context.append(related_node[\"id\"] + ': ' + related_node[\"text\"])\n",
    "    return full_context\n",
    "\n",
    "def get_answer(question, context, max_new_tokens, temperature):\n",
    "    query = QA_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "    chat_completion = llm_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=max_new_tokens\n",
    "    )\n",
    "    print(f'4. LLM based answer')\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "def generate_qa(msg, max_new_tokens, temperature, top_k):\n",
    "    similar_nodes = get_similar_nodes(msg, top_k)\n",
    "    most_relevant_node, most_relevant_txt = get_most_relevant_node(msg, similar_nodes)\n",
    "    full_context = build_context(most_relevant_node, most_relevant_txt)\n",
    "    return get_answer(msg, full_context, max_new_tokens, temperature)\n",
    "\n",
    "def generate_chat(msg, max_new_tokens, temperature, top_k, chat_history):\n",
    "    similar_nodes = get_similar_nodes(msg, top_k)\n",
    "    most_relevant_node, most_relevant_txt = get_most_relevant_node(msg, similar_nodes)\n",
    "    full_context = build_context(most_relevant_node, most_relevant_txt)\n",
    "    result = get_answer(msg, full_context, max_new_tokens, temperature)\n",
    "    chat_history.append((msg, result))\n",
    "    return \"\", chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973742c5-bc4b-4c7b-95fb-0062b143f9ec",
   "metadata": {},
   "source": [
    "## QA interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d7d703e-0572-400f-89d2-de6e36165a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://b4fa58ad350676c04d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b4fa58ad350676c04d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Retrieve data from vector store based on similarity:\n",
      "Similar nodes to the query [('4.1.', 0.6936294134818086), ('3.1.', 0.6661224263630892), ('3.1.1.', 0.6470114174978703)]\n",
      "2. LLM based decision on the most relevant nodeMost relevant node 4.1.:  If the vehicle type submitted for approval pursuant to this Regulation meets the requirements of paragraph 5 to 9 below, approval of that vehicle shall be granted. The functional operation of ‘The System’, as laid out in the documents required in paragraph 3, shall be tested as follows  Schematic system layout including sensors for the environmental perception (e.g. block diagram) … Parameters below are essential when describing the pattern of the traffic critical scenarios in section 2.1.\n",
      "3. Cypher query to generate the full context based on the most relevant node: [{'id': '5.', 'text': ' System Safety and Fail-safe Response Test Specifications for ALKS'}, {'id': '9.', 'text': ' Cybersecurity and Software Updates'}]\n",
      "4. LLM based answer\n"
     ]
    }
   ],
   "source": [
    "qa_interface = gr.Interface(fn=generate_qa, \n",
    "                            title=\"Graph RAG QA interface\",\n",
    "                            inputs=[gr.Textbox(label=\"Prompt\"),\n",
    "                                    gr.Slider(label=\"Max new tokens\", value=200, maximum=1024, step=int, minimum=1),\n",
    "                                    gr.Slider(label=\"Temperature\", value=0.8, maximum=1.0, minimum=0.0),\n",
    "                                    gr.Slider(label=\"Max documents retrieved\", value=3, step=int, maximum=5, minimum=1)\n",
    "                                   ], \n",
    "                            outputs=[gr.Textbox(label=\"Completion\")]\n",
    "                           )\n",
    "gr.close_all()\n",
    "qa_interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27696524-3b54-4e08-8392-efd7a9022c05",
   "metadata": {},
   "source": [
    "## Chatbot interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b6d80b4-de21-480b-a29e-153e7eba0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/moz9le/Documents/ml11/lib/python3.11/site-packages/gradio/components/chatbot.py:225: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "Closing server running on port: 7860\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://f326dcb3e293f037e8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f326dcb3e293f037e8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Retrieve data from vector store based on similarity:\n",
      "Similar nodes to the query [('4.1.', 0.6936294134818086), ('3.1.', 0.6661224263630892), ('3.1.1.', 0.6470114174978703)]\n",
      "2. LLM based decision on the most relevant nodeMost relevant node 4.1.:  If the vehicle type submitted for approval pursuant to this Regulation meets the requirements of paragraph 5 to 9 below, approval of that vehicle shall be granted. The functional operation of ‘The System’, as laid out in the documents required in paragraph 3, shall be tested as follows  Schematic system layout including sensors for the environmental perception (e.g. block diagram) … Parameters below are essential when describing the pattern of the traffic critical scenarios in section 2.1.\n",
      "3. Cypher query to generate the full context based on the most relevant node: [{'id': '5.', 'text': ' System Safety and Fail-safe Response Test Specifications for ALKS'}, {'id': '9.', 'text': ' Cybersecurity and Software Updates'}]\n",
      "4. LLM based answer\n",
      "1. Retrieve data from vector store based on similarity:\n",
      "Similar nodes to the query [('1.1.', 0.3963746675288466), ('4.4.2.', 0.3872126260648013), ('4.2.', 0.38400390191022354)]\n",
      "2. LLM based decision on the most relevant nodeMost relevant node 1.1.:  This Regulation applies to the type approval of vehicles of Category M1 (1) with regards to their Automated Lane Keeping System. Operational Design Domain (Speed, road type, country, Environment, Road conditions, etc.)/ Boundary conditions/ Main conditions for Minimum risk manoeuvres and transition demands … This document clarifies derivation process to define conditions under which Automated Lane Keeping Systems (ALKS) shall avoid a collision. Conditions under which ALKS shall avoid a collision are determined by a general simulation program with following attentive human driver performance model and related parameters in the traffic critical disturbance scenarios.\n",
      "3. Cypher query to generate the full context based on the most relevant node: []\n",
      "4. LLM based answer\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as chat_interface:\n",
    "    title = gr.HTML(\"<center><h1>Graph RAG based retrieval</h1></center>\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(label=\"Prompt\")\n",
    "            max_new_tokens = gr.Slider(label=\"Max new tokens\", value=200, maximum=1024, step=int, minimum=1)\n",
    "            temperature = gr.Slider(label=\"Temperature\", value=0.8, maximum=1.0, minimum=0.0)\n",
    "            max_documents = gr.Slider(label=\"Max documents retrieved\", value=3, step=int, maximum=5, minimum=1)\n",
    "        with gr.Column():\n",
    "            chatbot = gr.Chatbot(height=240)\n",
    "    btn = gr.Button(\"Query\")\n",
    "    clear = gr.ClearButton(components=[msg, chatbot], value=\"Clear console\")\n",
    "    btn.click(\n",
    "        generate_chat,\n",
    "        inputs=[\n",
    "            msg,\n",
    "            max_new_tokens,\n",
    "            temperature,\n",
    "            max_documents,\n",
    "            chatbot],\n",
    "        outputs=[msg, chatbot])\n",
    "gr.close_all()\n",
    "chat_interface.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
